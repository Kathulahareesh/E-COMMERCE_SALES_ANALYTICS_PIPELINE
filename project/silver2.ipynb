{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8af793a-101c-4f72-b187-ed2b8db8bda5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType, LongType, IntegerType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Configuration and Setup\n",
    "# Define the source S3 path for the Bronze Delta files\n",
    "source_catalog = \"bronze\"\n",
    "source_schema = \"e-commerce-sales\"\n",
    "source_path = f\"{source_catalog}.`{source_schema}`.\"\n",
    "\n",
    "# Define the target Unity Catalog and schema for the Silver layer\n",
    "target_catalog = \"silver\"\n",
    "target_schema = \"e-commerce-sales\"\n",
    "audit_log_table = \"audit_logs\"\n",
    "\n",
    "print(f\"Source Path: '{source_catalog}.{source_schema}'\")\n",
    "print(f\"Destination Schema: '{target_catalog}.{target_schema}'\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Audit Logging Function\n",
    "audit_log_table_full_name = f\"{target_catalog}.`{target_schema}`.`{audit_log_table}`\"\n",
    "\n",
    "def log_audit(table_name, status, initial_count=None, final_count=None, message=None):\n",
    "    \"\"\"\n",
    "    Logs a record to the audit_logs table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the schema for the log entry\n",
    "        log_schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), False),\n",
    "            StructField(\"table_name\", StringType(), False),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"initial_count\", LongType(), True),\n",
    "            StructField(\"final_count\", LongType(), True),\n",
    "            StructField(\"message\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Create the log entry data\n",
    "        log_data = [(\n",
    "            datetime.now(),\n",
    "            table_name,\n",
    "            status,\n",
    "            initial_count,\n",
    "            final_count,\n",
    "            message\n",
    "        )]\n",
    "        \n",
    "        # Create a DataFrame and append it to the audit log table\n",
    "        log_df = spark.createDataFrame(log_data, schema=log_schema)\n",
    "        log_df.write.format(\"delta\").mode(\"append\").saveAsTable(audit_log_table_full_name)\n",
    "        print(f\"Logged '{status}' for table '{table_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not write to audit log table. Error: {e}\")\n",
    "\n",
    "# Log the start of the entire Silver job\n",
    "log_audit(\"Silver Layer Job\", \"Started\", message=\"Silver layer processing job initiated.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Process 'distribution_centers'\n",
    "table_name = \"distribution_centers\"\n",
    "bronze_path = source_path + table_name\n",
    "silver_table_full_name = f\"{target_catalog}.`{target_schema}`.`{table_name}`\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(bronze_path)\n",
    "    initial_count = df.count()\n",
    "    log_audit(table_name, \"Started\", initial_count=initial_count)\n",
    "    \n",
    "    defaults = {\"id\": 0, \"name\": \"Unknown\", \"latitude\": 0.0, \"longitude\": 0.0}\n",
    "    df_filled = df.fillna(defaults)\n",
    "    \n",
    "    string_columns = [f.name for f in df_filled.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_columns:\n",
    "        df_filled = df_filled.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "        \n",
    "    df_deduplicated = df_filled.dropDuplicates([\"id\"])\n",
    "    final_count = df_deduplicated.count()\n",
    "    \n",
    "    df_deduplicated.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_audit(table_name, \"Success\", initial_count, final_count, f\"Successfully processed and saved to {silver_table_full_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print()\n",
    "    log_audit(table_name, \"Failed\", message=str(e))\n",
    "    \n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Process 'events'\n",
    "table_name = \"events\"\n",
    "bronze_path = source_path + table_name\n",
    "silver_table_full_name = f\"{target_catalog}.`{target_schema}`.`{table_name}`\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(bronze_path)\n",
    "    initial_count = df.count()\n",
    "    log_audit(table_name, \"Started\", initial_count=initial_count)\n",
    "    \n",
    "    defaults = {\n",
    "        \"id\": 0, \"user_id\": 0, \"sequence_number\": 0, \"session_id\": \"0\",\n",
    "        \"ip_address\": \"Unknown\", \"city\": \"Unknown\", \"state\": \"Unknown\",\n",
    "        \"postal_code\": \"Unknown\", \"browser\": \"Unknown\", \"traffic_source\": \"Unknown\",\n",
    "        \"uri\": \"Unknown\", \"event_type\": \"Unknown\"\n",
    "    }\n",
    "    df_filled = df.fillna(defaults)\n",
    "    df_filled = df_filled.withColumn(\"created_at\", F.when(F.col(\"created_at\").isNull(), F.current_timestamp()).otherwise(F.col(\"created_at\")))\n",
    "    \n",
    "    string_columns = [f.name for f in df_filled.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_columns:\n",
    "        df_filled = df_filled.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "        \n",
    "    df_deduplicated = df_filled.dropDuplicates([\"id\"])\n",
    "    final_count = df_deduplicated.count()\n",
    "    \n",
    "    df_deduplicated.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_audit(table_name, \"Success\", initial_count, final_count, f\"Successfully processed and saved to {silver_table_full_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_audit(table_name, \"Failed\", message=str(e))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Process 'inventory_items'\n",
    "table_name = \"inventory_items\"\n",
    "bronze_path = source_path + table_name\n",
    "silver_table_full_name = f\"{target_catalog}.`{target_schema}`.`{table_name}`\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(bronze_path)\n",
    "    initial_count = df.count()\n",
    "    log_audit(table_name, \"Started\", initial_count=initial_count)\n",
    "    \n",
    "    defaults = {\"id\": 0, \"product_id\": 0, \"cost\": 0.0, \"product_category\": \"Unknown\"}\n",
    "    df_filled = df.fillna(defaults)\n",
    "    \n",
    "    df_filled = df_filled.withColumn(\"created_at\", F.when(F.col(\"created_at\").isNull(), F.current_timestamp()).otherwise(F.col(\"created_at\")))\n",
    "    df_filled = df_filled.withColumn(\"sold_at\", F.when(F.col(\"sold_at\").isNull(), F.current_timestamp()).otherwise(F.col(\"sold_at\")))\n",
    "    \n",
    "    string_columns = [f.name for f in df_filled.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_columns:\n",
    "        df_filled = df_filled.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "        \n",
    "    df_deduplicated = df_filled.dropDuplicates([\"id\"])\n",
    "    final_count = df_deduplicated.count()\n",
    "    \n",
    "    df_deduplicated.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_audit(table_name, \"Success\", initial_count, final_count, f\"Successfully processed and saved to {silver_table_full_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_audit(table_name, \"Failed\", message=str(e))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Process 'order_items'\n",
    "table_name = \"order_items\"\n",
    "bronze_path = source_path + table_name\n",
    "silver_table_full_name = f\"{target_catalog}.`{target_schema}`.`{table_name}`\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(bronze_path)\n",
    "    initial_count = df.count()\n",
    "    log_audit(table_name, \"Started\", initial_count=initial_count)\n",
    "    \n",
    "    defaults = {\"id\": 0, \"order_id\": 0, \"user_id\": 0, \"product_id\": 0, \"inventory_item_id\": 0, \"status\": \"Unknown\", \"sale_price\": 0.0}\n",
    "    df_filled = df.fillna(defaults)\n",
    "    \n",
    "    timestamp_cols = [\"created_at\", \"shipped_at\", \"delivered_at\", \"returned_at\"]\n",
    "    for col_name in timestamp_cols:\n",
    "        df_filled = df_filled.withColumn(col_name, F.when(F.col(col_name).isNull(), F.current_timestamp()).otherwise(F.col(col_name)))\n",
    "    \n",
    "    string_columns = [f.name for f in df_filled.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_columns:\n",
    "        df_filled = df_filled.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "        \n",
    "    df_deduplicated = df_filled.dropDuplicates([\"id\"])\n",
    "    final_count = df_deduplicated.count()\n",
    "    \n",
    "    df_deduplicated.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_audit(table_name, \"Success\", initial_count, final_count, f\"Successfully processed and saved to {silver_table_full_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_audit(table_name, \"Failed\", message=str(e))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Process 'orders'\n",
    "table_name = \"orders\"\n",
    "bronze_path = source_path + table_name\n",
    "silver_table_full_name = f\"{target_catalog}.`{target_schema}`.`{table_name}`\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.table(bronze_path)\n",
    "    initial_count = df.count()\n",
    "    log_audit(table_name, \"Started\", initial_count=initial_count)\n",
    "    \n",
    "    defaults = {\"order_id\": 0, \"user_id\": 0, \"status\": \"Unknown\", \"gender\": \"Unknown\", \"num_of_item\": 0}\n",
    "    df_filled = df.fillna(defaults)\n",
    "    \n",
    "    timestamp_cols = [\"created_at\", \"shipped_at\", \"delivered_at\", \"returned_at\"]\n",
    "    for col_name in timestamp_cols:\n",
    "        df_filled = df_filled.withColumn(col_name, F.when(F.col(col_name).isNull(), F.current_timestamp()).otherwise(F.col(col_name)))\n",
    "    \n",
    "    string_columns = [f.name for f in df_filled.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_columns:\n",
    "        df_filled = df_filled.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "        \n",
    "    df_deduplicated = df_filled.dropDuplicates([\"order_id\"])\n",
    "    final_count = df_deduplicated.count()\n",
    "    \n",
    "    df_deduplicated.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_audit(table_name, \"Success\", initial_count, final_count, f\"Successfully processed and saved to {silver_table_full_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_audit(table_name, \"Failed\", message=str(e))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Process 'products'\n",
    "table_name = \"products\"\n",
    "bronze_path = source_path + table_name\n",
    "silver_table_full_name = f\"{target_catalog}.`{target_schema}`.`{table_name}`\"\n",
    "\n",
    "try:\n",
    "    df = spark.table(bronze_path)\n",
    "    initial_count = df.count()\n",
    "    log_audit(table_name, \"Started\", initial_count=initial_count)\n",
    "    \n",
    "    defaults = {\"id\": 0, \"cost\": 0.0, \"category\": \"Unknown\"}\n",
    "    df_filled = df.fillna(defaults)\n",
    "    \n",
    "    string_columns = [f.name for f in df_filled.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_columns:\n",
    "        df_filled = df_filled.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "        \n",
    "    df_deduplicated = df_filled.dropDuplicates([\"id\"])\n",
    "    final_count = df_deduplicated.count()\n",
    "    \n",
    "    df_deduplicated.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_audit(table_name, \"Success\", initial_count, final_count, f\"Successfully processed and saved to {silver_table_full_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "    log_audit(table_name, \"Failed\", message=str(e))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Process 'users'\n",
    "table_name = \"users\"\n",
    "bronze_path = source_path + table_name\n",
    "silver_table_full_name = f\"{target_catalog}.`{target_schema}`.`{table_name}`\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.table(bronze_path)\n",
    "    initial_count = df.count()\n",
    "    log_audit(table_name, \"Started\", initial_count=initial_count)\n",
    "    \n",
    "    defaults = {\n",
    "        \"id\": 0, \"first_name\": \"Unknown\", \"last_name\": \"Unknown\", \"email\": \"Unknown\",\n",
    "        \"age\": 0, \"gender\": \"Unknown\", \"state\": \"Unknown\", \"street_address\": \"Unknown\",\n",
    "        \"postal_code\": \"Unknown\", \"city\": \"Unknown\", \"country\": \"Unknown\",\n",
    "        \"latitude\": 0.0, \"longitude\": 0.0, \"traffic_source\": \"Unknown\"\n",
    "    }\n",
    "    df_filled = df.fillna(defaults)\n",
    "    \n",
    "    df_filled = df_filled.withColumn(\"created_at\", F.when(F.col(\"created_at\").isNull(), F.current_timestamp()).otherwise(F.col(\"created_at\")))\n",
    "    \n",
    "    string_columns = [f.name for f in df_filled.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_columns:\n",
    "        df_filled = df_filled.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "        \n",
    "    df_deduplicated = df_filled.dropDuplicates([\"id\"])\n",
    "    final_count = df_deduplicated.count()\n",
    "    \n",
    "    df_deduplicated.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(silver_table_full_name)\n",
    "    log_audit(table_name, \"Success\", initial_count, final_count, f\"Successfully processed and saved to {silver_table_full_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_audit(table_name, \"Failed\", message=str(e))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "display(spark.table(silver_table_full_name))\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Finalize Job\n",
    "log_audit(\"Silver Layer Job\", \"Finished\", message=\"Silver layer processing job completed.\")\n",
    "\n",
    "\n",
    "# Preview the `audit_logs` table to check the results.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(spark.table(audit_log_table_full_name).orderBy(F.desc(\"timestamp\")))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
