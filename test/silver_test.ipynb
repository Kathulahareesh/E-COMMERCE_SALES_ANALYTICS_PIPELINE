{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc37e783-1219-421c-ab77-0efaea9acfec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType, LongType\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Step 1: Setup - Create ALL Mock Bronze Tables in a Temporary Schema\n",
    "# Create a unique schema name to avoid conflicts\n",
    "test_run_id = str(uuid.uuid4()).replace(\"-\", \"_\")\n",
    "temp_catalog = \"hive_metastore\" \n",
    "temp_schema = f\"silver_test_workspace_{test_run_id}\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {temp_catalog}.{temp_schema}\")\n",
    "print(f\"Temporary test schema created: {temp_catalog}.{temp_schema}\")\n",
    "\n",
    "# --- Define all necessary schemas ---\n",
    "dist_centers_schema = StructType([StructField(\"id\", IntegerType()), StructField(\"name\", StringType()), StructField(\"latitude\", DoubleType()), StructField(\"longitude\", DoubleType())])\n",
    "events_schema = StructType([StructField(\"id\", IntegerType()), StructField(\"user_id\", IntegerType()), StructField(\"sequence_number\", LongType()), StructField(\"session_id\", StringType()), StructField(\"created_at\", TimestampType()), StructField(\"ip_address\", StringType()), StructField(\"city\", StringType()), StructField(\"state\", StringType()), StructField(\"postal_code\", StringType()), StructField(\"browser\", StringType()), StructField(\"traffic_source\", StringType()), StructField(\"uri\", StringType()), StructField(\"event_type\", StringType())])\n",
    "inventory_items_schema = StructType([StructField(\"id\", IntegerType()), StructField(\"product_id\", IntegerType()), StructField(\"created_at\", TimestampType()), StructField(\"sold_at\", TimestampType()), StructField(\"cost\", DoubleType()), StructField(\"product_category\", StringType())])\n",
    "order_items_schema = StructType([StructField(\"id\", IntegerType()), StructField(\"order_id\", IntegerType()), StructField(\"user_id\", IntegerType()), StructField(\"product_id\", IntegerType()), StructField(\"inventory_item_id\", IntegerType()), StructField(\"status\", StringType()), StructField(\"created_at\", TimestampType()), StructField(\"shipped_at\", TimestampType()), StructField(\"delivered_at\", TimestampType()), StructField(\"returned_at\", TimestampType()), StructField(\"sale_price\", DoubleType())])\n",
    "orders_schema = StructType([StructField(\"order_id\", IntegerType()), StructField(\"user_id\", IntegerType()), StructField(\"status\", StringType()), StructField(\"gender\", StringType()), StructField(\"created_at\", TimestampType()), StructField(\"returned_at\", TimestampType()), StructField(\"shipped_at\", TimestampType()), StructField(\"delivered_at\", TimestampType()), StructField(\"num_of_item\", IntegerType())])\n",
    "products_schema = StructType([StructField(\"id\", IntegerType()), StructField(\"cost\", DoubleType()), StructField(\"category\", StringType())])\n",
    "users_schema = StructType([StructField(\"id\", IntegerType()), StructField(\"first_name\", StringType()), StructField(\"last_name\", StringType()), StructField(\"email\", StringType()), StructField(\"age\", IntegerType()), StructField(\"gender\", StringType()), StructField(\"state\", StringType()), StructField(\"street_address\", StringType()), StructField(\"postal_code\", StringType()), StructField(\"city\", StringType()), StructField(\"country\", StringType()), StructField(\"latitude\", DoubleType()), StructField(\"longitude\", DoubleType()), StructField(\"traffic_source\", StringType()), StructField(\"created_at\", TimestampType())])\n",
    "\n",
    "# --- Create mock tables WITH DATA for testing ---\n",
    "dist_centers_data = [(1, \"Center A \", 34.05, -118.24), (2, \"Center B\", None, -73.93), (3, None, 40.71, -74.00), (1, \"Center A Duplicate\", 34.05, -118.24)]\n",
    "spark.createDataFrame(dist_centers_data, dist_centers_schema).write.mode(\"overwrite\").saveAsTable(f\"{temp_catalog}.{temp_schema}.distribution_centers\")\n",
    "\n",
    "users_data = [(101, \" John \", \"Doe\", \"j.doe@email.com\", 30, \"Male\", \"CA\", None, \"90210\", \"LA\", \"USA\", 34.05, -118.24, \"google\", datetime(2023, 1, 1)), (102, \"Jane\", None, \"jane@email.com\", 25, \"Female\", \"NY\", \"123 Main\", \"10001\", \"NYC\", \"USA\", 40.71, -74.00, \"facebook\", None), (101, \"John\", \"Doe\", \"j.doe2@email.com\", 31, \"Male\", \"CA\", \"456 Oak\", \"90210\", \"LA\", \"USA\", 34.05, -118.24, \"google\", datetime(2023, 1, 2))]\n",
    "spark.createDataFrame(users_data, users_schema).write.mode(\"overwrite\").saveAsTable(f\"{temp_catalog}.{temp_schema}.users\")\n",
    "\n",
    "orders_data = [(201, 101, \"Shipped \", \"Male\", datetime(2023, 1, 1), None, None, None, 2), (202, 102, \"Processing\", \"Female\", datetime(2023, 1, 2), None, None, None, 1), (201, 101, \"Delivered\", \"Male\", datetime(2023, 1, 1), None, None, None, 2)]\n",
    "spark.createDataFrame(orders_data, orders_schema).write.mode(\"overwrite\").saveAsTable(f\"{temp_catalog}.{temp_schema}.orders\")\n",
    "\n",
    "products_data = [(301, 10.50, \"Books \"), (302, None, \"Electronics\"), (301, 12.00, \"Books\")]\n",
    "spark.createDataFrame(products_data, products_schema).write.mode(\"overwrite\").saveAsTable(f\"{temp_catalog}.{temp_schema}.products\")\n",
    "\n",
    "# --- Create other required tables as EMPTY tables to allow the notebook to run ---\n",
    "spark.createDataFrame([], events_schema).write.mode(\"overwrite\").saveAsTable(f\"{temp_catalog}.{temp_schema}.events\")\n",
    "spark.createDataFrame([], inventory_items_schema).write.mode(\"overwrite\").saveAsTable(f\"{temp_catalog}.{temp_schema}.inventory_items\")\n",
    "spark.createDataFrame([], order_items_schema).write.mode(\"overwrite\").saveAsTable(f\"{temp_catalog}.{temp_schema}.order_items\")\n",
    "\n",
    "print(\"All mock Bronze tables created.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 2,Step 2: Define Expected Silver Outputs\n",
    "# --- Expected output for 'distribution_centers' ---\n",
    "expected_dist_centers_schema = dist_centers_schema\n",
    "expected_dist_centers_data1 = [(1, \"Center A\", 34.05, -118.24), (2, \"Center B\", 0.0, -73.93), (3, \"Unknown\", 40.71, -74.00)]\n",
    "expected_dist_centers_data2 = [(1, \"Center A Duplicate\", 34.05, -118.24), (2, \"Center B\", 0.0, -73.93), (3, \"Unknown\", 40.71, -74.00)]\n",
    "expected_dist_centers_df1 = spark.createDataFrame(expected_dist_centers_data1, expected_dist_centers_schema)\n",
    "expected_dist_centers_df2 = spark.createDataFrame(expected_dist_centers_data2, expected_dist_centers_schema)\n",
    "\n",
    "# --- Expected output for 'users' ---\n",
    "expected_users_schema = users_schema\n",
    "expected_users_data = [(101, \"John\", \"Doe\", \"j.doe@email.com\", 30, \"Male\", \"CA\", \"Unknown\", \"90210\", \"LA\", \"USA\", 34.05, -118.24, \"google\", datetime(2023, 1, 1)), (102, \"Jane\", \"Unknown\", \"jane@email.com\", 25, \"Female\", \"NY\", \"123 Main\", \"10001\", \"NYC\", \"USA\", 40.71, -74.00, \"facebook\", None)]\n",
    "expected_users_df = spark.createDataFrame(expected_users_data, expected_users_schema)\n",
    "\n",
    "# --- Expected output for 'orders' ---\n",
    "expected_orders_schema = orders_schema\n",
    "expected_orders_data = [(201, 101, \"Shipped\", \"Male\", datetime(2023, 1, 1), None, None, None, 2), (202, 102, \"Processing\", \"Female\", datetime(2023, 1, 2), None, None, None, 1)]\n",
    "expected_orders_df = spark.createDataFrame(expected_orders_data, expected_orders_schema)\n",
    "\n",
    "# --- Expected output for 'products' ---\n",
    "expected_products_schema = products_schema\n",
    "expected_products_data = [(301, 10.50, \"Books\"), (302, 0.0, \"Electronics\")]\n",
    "expected_products_df = spark.createDataFrame(expected_products_data, expected_products_schema)\n",
    "\n",
    "print(\"All Expected Silver DataFrames defined.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 3,Step 3: Execute the Main Notebook\n",
    "test_results = {}\n",
    "try:\n",
    "    print(\"Executing the main Silver Layer notebook...\")\n",
    "    # !!! IMPORTANT !!! \n",
    "    # Replace the path below with the correct path to your main Silver Layer notebook.\n",
    "    notebook_to_test = \"/Workspace/final_project/project/silver2\" \n",
    "    \n",
    "    params = {\n",
    "        \"source_catalog\": temp_catalog, \"source_schema\": temp_schema,\n",
    "        \"target_catalog\": temp_catalog, \"target_schema\": temp_schema \n",
    "    }\n",
    "    \n",
    "    dbutils.notebook.run(notebook_to_test, 600, params)\n",
    "    print(\"Main notebook execution complete.\")\n",
    "\n",
    "    # COMMAND ----------\n",
    "    \n",
    "    # DBTITLE 4,Step 4: Assert Results for Each Table\n",
    "    \n",
    "    # --- Assert 'distribution_centers' ---\n",
    "    try:\n",
    "        print(\"\\n--- ASSERTING 'distribution_centers' ---\")\n",
    "        actual_df = spark.table(f\"{temp_catalog}.{temp_schema}.distribution_centers\")\n",
    "        actual_pd = actual_df.sort(\"id\").toPandas()\n",
    "        expected_pd1 = expected_dist_centers_df1.sort(\"id\").toPandas()\n",
    "        expected_pd2 = expected_dist_centers_df2.sort(\"id\").toPandas()\n",
    "        if not (actual_pd.equals(expected_pd1) or actual_pd.equals(expected_pd2)):\n",
    "            raise AssertionError(\"Actual output did not match expected outputs.\")\n",
    "        print(\"✅ TEST PASSED: 'distribution_centers'\")\n",
    "        test_results['distribution_centers'] = 'PASSED'\n",
    "    except Exception as e:\n",
    "        print(f\"❌ TEST FAILED: 'distribution_centers'\"); test_results['distribution_centers'] = f'FAILED: {e}'\n",
    "\n",
    "    # --- Assert 'users' ---\n",
    "    try:\n",
    "        print(\"\\n--- ASSERTING 'users' ---\")\n",
    "        actual_df = spark.table(f\"{temp_catalog}.{temp_schema}.users\")\n",
    "        assert actual_df.filter(\"id = 102\").select(\"created_at\").first()[0] is not None, \"Timestamp for user 102 should be filled\"\n",
    "        actual_pd = actual_df.drop(\"created_at\").sort(\"id\").toPandas()\n",
    "        expected_pd = expected_users_df.drop(\"created_at\").sort(\"id\").toPandas()\n",
    "        pd.testing.assert_frame_equal(actual_pd, expected_pd)\n",
    "        print(\"✅ TEST PASSED: 'users'\"); test_results['users'] = 'PASSED'\n",
    "    except Exception as e:\n",
    "        print(f\"❌ TEST FAILED: 'users'\"); test_results['users'] = f'FAILED: {e}'\n",
    "\n",
    "    # --- Assert 'orders' ---\n",
    "    try:\n",
    "        print(\"\\n--- ASSERTING 'orders' ---\")\n",
    "        actual_df = spark.table(f\"{temp_catalog}.{temp_schema}.orders\")\n",
    "        \n",
    "        # New, more specific assertions for timestamps\n",
    "        filled_timestamps = actual_df.filter(\"order_id = 201\").select(\"returned_at\", \"shipped_at\", \"delivered_at\").first()\n",
    "        assert filled_timestamps.returned_at is not None, \"The 'returned_at' column for order 201 should have been filled.\"\n",
    "        assert filled_timestamps.shipped_at is not None, \"The 'shipped_at' column for order 201 should have been filled.\"\n",
    "        assert filled_timestamps.delivered_at is not None, \"The 'delivered_at' column for order 201 should have been filled.\"\n",
    "\n",
    "        # Compare the rest of the data after dropping non-deterministic columns\n",
    "        actual_pd = actual_df.drop(\"returned_at\", \"shipped_at\", \"delivered_at\").sort(\"order_id\").toPandas()\n",
    "        expected_pd = expected_orders_df.drop(\"returned_at\", \"shipped_at\", \"delivered_at\").sort(\"order_id\").toPandas()\n",
    "        pd.testing.assert_frame_equal(actual_pd, expected_pd)\n",
    "        print(\"✅ TEST PASSED: 'orders'\"); test_results['orders'] = 'PASSED'\n",
    "    except Exception as e:\n",
    "        print(f\"❌ TEST FAILED: 'orders'\"); test_results['orders'] = f'FAILED: {e}'\n",
    "\n",
    "    # --- Assert 'products' ---\n",
    "    try:\n",
    "        print(\"\\n--- ASSERTING 'products' ---\")\n",
    "        actual_df = spark.table(f\"{temp_catalog}.{temp_schema}.products\")\n",
    "        actual_pd = actual_df.sort(\"id\").toPandas()\n",
    "        expected_pd = expected_products_df.sort(\"id\").toPandas()\n",
    "        pd.testing.assert_frame_equal(actual_pd, expected_pd)\n",
    "        print(\"✅ TEST PASSED: 'products'\"); test_results['products'] = 'PASSED'\n",
    "    except Exception as e:\n",
    "        print(f\"❌ TEST FAILED: 'products'\"); test_results['products'] = f'FAILED: {e}'\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ TEST FAILED: An error occurred during the main notebook run.\")\n",
    "    print(\"\\n----- ERROR DETAILS -----\"); print(e)\n",
    "\n",
    "finally:\n",
    "    # --- CLEANUP ---\n",
    "    print(\"\\n--- FINAL TEST SUMMARY ---\")\n",
    "    for table, result in test_results.items():\n",
    "        print(f\"- {table}: {result}\")\n",
    "    \n",
    "    print(f\"\\nCleaning up: Dropping temporary schema '{temp_catalog}.{temp_schema}'...\")\n",
    "    spark.sql(f\"DROP SCHEMA IF EXISTS {temp_catalog}.{temp_schema} CASCADE\")\n",
    "    print(\"Cleanup complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
